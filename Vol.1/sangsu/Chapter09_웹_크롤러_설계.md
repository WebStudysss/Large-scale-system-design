**크롤러의 사용 용도**

- 검색 엔진 인덱싱
- 웹 아카이빙
- 웹 마이닝
- 웹 모니터링

**크롤러 요구사항**

- 규모 확장성
- 안정성
- 예절
- 확장성

**규모 추정**

- 매달 10억개
- QPS = 10억/30일/24시간/3600초 : 초당 400페이지
- 최대 QPS = 2 * QPS = 800
- 웹 페이지 크기 평균 500K라고 가정
- 10억 페이지 * 500K = 500TB/월
- 5년 보관일 때 필요 용량 : 500TB * 12 * 6 : 30PB

**설계안 준비**

시작 URL 집합 → 미수집 URL 저장소 → HTML 다운로더 → 도메인 이름 변환기, 컨텐츠 파서 → 중복 컨텐츠 필터링 → URL 추출기 → URL 필터 → URL 저장소

- **시작 URL 집합**: 크롤링을 시작할 시드(초기) URL 목록
- **미수집 URL 저장소**: 아직 방문하지 않은 URL을 보관,스케줄링하는 큐/프론티어
- **HTML 다운로더**: URL에 요청을 보내 HTML/리소스를 내려받는 모듈(재시도·타임아웃 포함)
- **도메인 이름 변환기**: IDN(한글 도메인) → Punycode, URL 정규화/정 canon 처리
- **콘텐츠 파서**: HTML을 DOM으로 파싱하고 텍스트·메타데이터를 추출
- **중복 콘텐츠 선별**: SimHash/셔링 등으로 이미 본 문서와 유사·중복 여부 판단
- **콘텐츠 저장소**: 원문/정제 텍스트와 메타를 영구 저장(버전·압축·인덱싱)
- **URL 추출기**: 파싱된 문서에서 링크(rel, a/img/script/src 등) 수집
- **URL 필터**: robots.txt/규칙/도메인·경로·MIME·중복 규칙으로 수집 여부 판단
- **이미 방문한 URL**: 방문 이력(해시셋/블룸필터)로 재방문 방지
- **URL 저장소**: 최종 크롤 대상 URL과 상태(대기/실패/완료) 메타 관리

### 전반적인 작업 흐름

웹 크롤링 시에는 DFS, BFS 기법 중 BFS 알고리즘을 채택하는 경향이 있음. 이유는 폭 넓게 데이터를 파악해야 하기 때문

다시 한번 웹 크롤러의 특성을 보자

- 규모 확장성
- 안정성
- 예절
- 확장성

규모 확장성은 BFS 알고리즘을 활용해 URL을 수집,
안정성은 HTML 등 콘텐츠 다운로드를 할 때 각 서버를 병렬로 수집하도록 하여 안정적으로 동작하도록 구현
예절은 미수집 URL 저장소에 한 페이지는 한번의 요청만 할 수 있도록 다양한 기법을 통해 처리
확장성은 콘텐츠 저장을 할 때 다양한 모듈을 보내 특정 태그를 다운로드 받을 수 있도록 구현하면 될 것이다.
